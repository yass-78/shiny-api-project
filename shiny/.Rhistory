# Train a logistic regression model on the training dataset
model <- train(Osteoporosis ~ ., data = trainData, method = "glm", family = "binomial")
# Summarize the trained model
summary(model)
# Predict probabilities and classes for the validation dataset
probs <- predict(model, newdata = valData, type = "prob") # Generates probabilities
predictions <- predict(model, newdata = valData) # Class predictions
# Evaluate the model's performance using a confusion matrix
cm <- confusionMatrix(predictions, valData$Osteoporosis)
print("Confusion Matrix:")
print(cm) # Outputs the confusion matrix
# Convert the confusion matrix into a data frame for visualization
cm_df <- as.data.frame(as.table(cm))
colnames(cm_df) <- c("Predicted", "Actual", "Freq")
# Plot the confusion matrix
ggplot(cm_df, aes(x = Actual, y = Predicted, fill = Freq)) +
geom_tile(color = "white") +
geom_text(aes(label = Freq), color = "black", size = 4) +
scale_fill_gradient(low = "lightblue", high = "blue") +
labs(title = "Confusion Matrix", x = "Actual Class", y = "Predicted Class") +
theme_minimal()
# Generate and plot the ROC curve
roc_curve <- roc(valData$Osteoporosis, probs$Yes) # Generate ROC curve data
ggplot(data.frame(x = roc_curve$specificities, y = roc_curve$sensitivities), aes(x, y)) +
geom_line(color = "blue") +
labs(title = "ROC Curve", x = "1 - Specificity", y = "Sensitivity") +
theme_minimal()
# Argumentation of the ROC curve results:
# - A perfect model would have the curve reaching the top left corner (0, 1).
# - The steeper the curve, the better the model distinguishes between classes.
# - AUC is a summary measure of the ROC curve:
#   - AUC = 0.5: Random guessing.
#   - AUC = 1: Perfect classification.
# Generate the Precision-Recall Curve
pr_curve <- pr.curve(scores.class0 = probs$Yes, weights.class0 = as.numeric(valData$Osteoporosis) - 1, curve = TRUE)
# Convert PR curve data to a data frame
df <- data.frame(
recall = pr_curve$curve[,1],
precision = pr_curve$curve[,2]
)
# Plot the Precision-Recall Curve
ggplot(df, aes(x = recall, y = precision)) +
geom_line(color = "red", size = 1) +
labs(title = "Precision-Recall Curve", x = "Recall", y = "Precision") +
theme_minimal()
# Feature Importance: Identify the most influential variables
# Train a Random Forest model to extract feature importance
modelRF <- randomForest(Osteoporosis ~ ., data = trainData)
# Extract and visualize feature importance
importance_values <- importance(modelRF)
importance_df <- data.frame(Feature = rownames(importance_values), Importance = importance_values[, 1])
# Plot feature importance using ggplot2
ggplot(importance_df, aes(x = reorder(Feature, Importance), y = Importance)) +
geom_bar(stat = "identity", fill = "steelblue") +
coord_flip() + # Flips the x and y axes for better readability
labs(title = "Feature Importance", x = "Feature", y = "Importance") +
theme_minimal()
# Save the trained models for future use
save(model, file = "logistic_model.RData")
save(modelRF, file = "random_forest_model.RData")
# List files in the directory to confirm saved models
list.files()
# Load the necessary library for creating REST APIs in R
library(plumber)
# Load the API script located at the specified path
# The `plumb` function reads the R script and prepares it for use as an API.
pr <- plumb("API.R")
#Install required packages for data handling, model training, and evaluation
#install.packages("PRROC")
#install.packages("pROC")
#install.packages("smotefamily")
#install.packages("randomForest")
# Load the necessary libraries
library(caret)    # For model training and validation
library(dplyr)     # For data manipulation
library(smotefamily) # For oversampling techniques
library(ggplot2) # For data visualization
library(pROC)    # For ROC curve analysis
library(randomForest)# For Random Forest algorithm
library(PRROC)    # For Precision-Recall Curves
#path <- "C:/Users/julia/OneDrive/Documentos/Treball bioinfo/"
# Load the dataset ensuring that is saved at the specified path
data <- read.csv("osteoporosis.csv")
# Remove irrelevant columns that do not contribute to prediction such as "Id
data <- data %>% select(-Id, -Medical.Conditions)
# Preview the first few rows of the dataset
head(data)
# Check for missing values in the dataset
sum(is.na(data)) # Returns the number of missing values
# Check column names
colnames(data)
# Convert categorical variables to factors
data$Gender <- as.factor(data$Gender)
data$Hormonal.Changes <- as.factor(data$Hormonal.Changes)
data$Family.History <- as.factor(data$Family.History)
data$Race.Ethnicity <- as.factor(data$Race.Ethnicity)
data$Body.Weight <- as.factor(data$Body.Weight)
data$Calcium.Intake <- as.factor(data$Calcium.Intake)
data$Vitamin.D.Intake <- as.factor(data$Vitamin.D.Intake)
data$Physical.Activity <- as.factor(data$Physical.Activity)
data$Smoking <- as.factor(data$Smoking)
data$Alcohol.Consumption <- as.factor(data$Alcohol.Consumption)
data$Medications <- as.factor(data$Medications)
data$Prior.Fractures <- as.factor(data$Prior.Fractures)
# Convert the target variable (Osteoporosis) into a factor
# Replace numerical values (1, 0) with "Yes" and "No"
data$Osteoporosis <- ifelse(data$Osteoporosis == 1, "Yes", "No")
data$Osteoporosis <- as.factor(data$Osteoporosis)
# Split the dataset into training (70%) and validation (30%) sets
set.seed(123) # Set seed for reproducibility
trainIndex <- createDataPartition(data$Osteoporosis, p = 0.7, list = FALSE)
trainData <- data[trainIndex, ] # Training dataset
valData <- data[-trainIndex, ] # Validation dataset
# Check the distribution of the target variable in both datasets
table(trainData$Osteoporosis) # Count of each class in training data
table(valData$Osteoporosis) # Count of each class in validation data
# Ensure that the levels of the target variable are consistent across both datasets
levels(valData$Osteoporosis) <- levels(trainData$Osteoporosis)
# Train a logistic regression model on the training dataset
model <- train(Osteoporosis ~ ., data = trainData, method = "glm", family = "binomial")
# Summarize the trained model
summary(model)
# Predict probabilities and classes for the validation dataset
probs <- predict(model, newdata = valData, type = "prob") # Generates probabilities
predictions <- predict(model, newdata = valData) # Class predictions
# Evaluate the model's performance using a confusion matrix
cm <- confusionMatrix(predictions, valData$Osteoporosis)
print("Confusion Matrix:")
print(cm) # Outputs the confusion matrix
# Convert the confusion matrix into a data frame for visualization
cm_df <- as.data.frame(as.table(cm))
colnames(cm_df) <- c("Predicted", "Actual", "Freq")
# Plot the confusion matrix
ggplot(cm_df, aes(x = Actual, y = Predicted, fill = Freq)) +
geom_tile(color = "white") +
geom_text(aes(label = Freq), color = "black", size = 4) +
scale_fill_gradient(low = "lightblue", high = "blue") +
labs(title = "Confusion Matrix", x = "Actual Class", y = "Predicted Class") +
theme_minimal()
# Generate and plot the ROC curve
roc_curve <- roc(valData$Osteoporosis, probs$Yes) # Generate ROC curve data
ggplot(data.frame(x = roc_curve$specificities, y = roc_curve$sensitivities), aes(x, y)) +
geom_line(color = "blue") +
labs(title = "ROC Curve", x = "1 - Specificity", y = "Sensitivity") +
theme_minimal()
# Argumentation of the ROC curve results:
# - A perfect model would have the curve reaching the top left corner (0, 1).
# - The steeper the curve, the better the model distinguishes between classes.
# - AUC is a summary measure of the ROC curve:
#   - AUC = 0.5: Random guessing.
#   - AUC = 1: Perfect classification.
# Generate the Precision-Recall Curve
pr_curve <- pr.curve(scores.class0 = probs$Yes, weights.class0 = as.numeric(valData$Osteoporosis) - 1, curve = TRUE)
# Convert PR curve data to a data frame
df <- data.frame(
recall = pr_curve$curve[,1],
precision = pr_curve$curve[,2]
)
# Plot the Precision-Recall Curve
ggplot(df, aes(x = recall, y = precision)) +
geom_line(color = "red", size = 1) +
labs(title = "Precision-Recall Curve", x = "Recall", y = "Precision") +
theme_minimal()
# Feature Importance: Identify the most influential variables
# Train a Random Forest model to extract feature importance
modelRF <- randomForest(Osteoporosis ~ ., data = trainData)
# Extract and visualize feature importance
importance_values <- importance(modelRF)
importance_df <- data.frame(Feature = rownames(importance_values), Importance = importance_values[, 1])
# Plot feature importance using ggplot2
ggplot(importance_df, aes(x = reorder(Feature, Importance), y = Importance)) +
geom_bar(stat = "identity", fill = "steelblue") +
coord_flip() + # Flips the x and y axes for better readability
labs(title = "Feature Importance", x = "Feature", y = "Importance") +
theme_minimal()
# Save the trained models for future use
save(model, file = paste0(path, "logistic_model.RData"))
# Load necessary libraries
library(plumber)  # For creating REST APIs in R
library(caret)  # For building predictive models
library(dplyr)   # For data manipulation
# Load the pre-trained logistic regression model
load("C:/Users/julia/OneDrive/Documentos/Treball bioinfo/logistic_model.RData")
# Load necessary libraries
library(plumber)  # For creating REST APIs in R
library(caret)  # For building predictive models
library(dplyr)   # For data manipulation
# Load the pre-trained logistic regression model
load("logistic_model.RData")
# Define expected levels for each factor (categorical variable)
levels_gender <- c("Male", "Female")
levels_family_history <- c("Yes", "No")
levels_hormonal_changes <- c("Normal", "Postmenopausal")
levels_race_ethnicity <- c("Caucasian", "Asian", "African American")
levels_body_weight <- c("Normal", "Underweight")
levels_calcium_intake <- c("Low", "Adequate")
levels_vitamin_d_intake <- c("Sufficient", "Insufficient")
levels_physical_activity <- c("Sedentary", "Active")
levels_smoking <- c("Yes", "No")
levels_alcohol_consumption <- c("Moderate", "None")
levels_medications <- c("Corticosteroids", "None")
levels_prior_fractures <- c("Yes", "No")
# Endpoint for prediction
#* @apiTitle Osteoporosis Prediction API
#* @apiDescription This API predicts the likelihood of osteoporosis based on clinical and lifestyle factors.
#*                 It uses a logistic regression model trained on relevant patient data.
#*                 <br><br>
#*                 <br><b>INPUTS:</b><br>
#*                 <br> - <b>Age:</b> Age of the patient in years (e.g., 78).
#*                 <br> - <b>Gender:</b> Gender of the patient. Possible values: "Male", "Female".
#*                 <br> - <b>HormonalChanges:</b> Hormonal changes of the patient. Possible values: "Normal", "Postmenopausal".
#*                 <br> - <b>FamilyHistory:</b> Family history of osteoporosis. Possible values: "Yes", "No".
#*                 <br> - <b>RaceEthnicity:</b> Race or ethnicity of the patient. Possible values: "Caucasian", "Asian", "African American".
#*                 <br> - <b>BodyWeight:</b> Body weight category. Possible values: "Normal", "Underweight".
#*                 <br> - <b>CalciumIntake:</b> Calcium intake level. Possible values: "Low", "Adequate".
#*                 <br> - <b>VitaminDIntake:</b> Vitamin D intake level. Possible values: "Sufficient", "Insufficient".
#*                 <br> - <b>PhysicalActivity:</b> Physical activity level. Possible values: "Sedentary", "Active".
#*                 <br> - <b>Smoking:</b> Smoking status. Possible values: "Yes", "No".
#*                 <br> - <b>AlcoholConsumption:</b> Alcohol consumption level. Possible values: "Moderate", "None".
#*                 <br> - <b>Medications:</b> Medication type. Possible values: "Corticosteroids", "None".
#*                 <br> - <b>PriorFractures:</b> History of prior fractures. Possible values: "Yes", "No".
#* @param Gender  Gender of the patient. Possible values: "Male", "Female".
#* @param Age  Age of the patient in years (e.g., 78).
#* @param HormonalChanges  Hormonal changes of the patient. Possible values: "Normal", "Postmenopausal".
#* @param FamilyHistory  Family history of osteoporosis. Possible values: "Yes", "No".
#* @param RaceEthnicity  Race or ethnicity of the patient. Possible values: "Caucasian", "Asian", "African American".
#* @param BodyWeight  Body weight category. Possible values: "Normal", "Underweight".
#* @param CalciumIntake  Calcium intake level. Possible values: "Low", "Adequate".
#* @param VitaminDIntake  Vitamin D intake level. Possible values: "Sufficient", "Insufficient".
#* @param PhysicalActivity  Physical activity level. Possible values: "Sedentary", "Active".
#* @param Smoking  Smoking status. Possible values: "Yes", "No".
#* @param AlcoholConsumption  Alcohol consumption level. Possible values: "Moderate", "None".
#* @param Medications  Medication type. Possible values: "Corticosteroids", "None".
#* @param PriorFractures  History of prior fractures. Possible values: "Yes", "No".
#* @post /predict_logistic
predict_logistic <- function(Gender, Age, HormonalChanges, FamilyHistory, RaceEthnicity, BodyWeight,
CalciumIntake, VitaminDIntake, PhysicalActivity,
Smoking, AlcoholConsumption, Medications, PriorFractures) {
# Create a data frame with the input data
input_data <- data.frame(
Gender = as.factor(Gender),
Age = as.numeric(Age),
Hormonal.Changes = as.factor(HormonalChanges),
Family.History = as.factor(FamilyHistory),
Race.Ethnicity = as.factor(RaceEthnicity),
Body.Weight = as.factor(BodyWeight),
Calcium.Intake = as.factor(CalciumIntake),
Vitamin.D.Intake = as.factor(VitaminDIntake),
Physical.Activity = as.factor(PhysicalActivity),
Smoking = as.factor(Smoking),
Alcohol.Consumption = as.factor(AlcoholConsumption),
Medications = as.factor(Medications),
Prior.Fractures = as.factor(PriorFractures)
)
# Adjust the levels of the factors to match expected values
levels(input_data$Gender) <- levels_gender
levels(input_data$Family.History) <- levels_family_history
levels(input_data$Race.Ethnicity) <- levels_race_ethnicity
levels(input_data$Body.Weight) <- levels_body_weight
levels(input_data$Calcium.Intake) <- levels_calcium_intake
levels(input_data$Vitamin.D.Intake) <- levels_vitamin_d_intake
levels(input_data$Physical.Activity) <- levels_physical_activity
levels(input_data$Smoking) <- levels_smoking
levels(input_data$Alcohol.Consumption) <- levels_alcohol_consumption
levels(input_data$Medications) <- levels_medications
levels(input_data$Prior.Fractures) <- levels_prior_fractures
# Make predictions using the pre-trained model
probs <- predict(model, newdata = input_data, type = "prob") # Probability prediction
prediction <- predict(model, newdata = input_data) # Class prediction
# Return the prediction results
list(
probability = probs$Yes, # Probability of having osteoporosis
prediction = as.character(prediction) # Predicted class ("Yes" or "No")
)
}
# Load the necessary library for creating REST APIs in R
library(plumber)
# Load the API script located at the specified path
# The `plumb` function reads the R script and prepares it for use as an API.
pr <- plumb("API.R")
# Run the API on port 8000
# The `run` method starts a local server on the specified port, allowing access to the API.
pr$run(port = 8000)
# Load the necessary library for creating REST APIs in R
library(plumber)
# Load the API script located at the specified path
# The `plumb` function reads the R script and prepares it for use as an API.
pr <- plumb("API.R")
# Run the API on port 8000
# The `run` method starts a local server on the specified port, allowing access to the API.
pr$run(port = 8000)
#Install required packages for data handling, model training, and evaluation
#install.packages("PRROC")
#install.packages("pROC")
#install.packages("smotefamily")
#install.packages("randomForest")
# Load the necessary libraries
library(caret)    # For model training and validation
library(dplyr)     # For data manipulation
library(smotefamily) # For oversampling techniques
library(ggplot2) # For data visualization
library(pROC)    # For ROC curve analysis
library(randomForest)# For Random Forest algorithm
library(PRROC)    # For Precision-Recall Curves
# Load the dataset ensuring that is saved at the specified path
data <- read.csv("osteoporosis.csv")
# Remove irrelevant columns that do not contribute to prediction such as "Id
data <- data %>% select(-Id, -Medical.Conditions)
# Preview the first few rows of the dataset
head(data)
# Check for missing values in the dataset
sum(is.na(data)) # Returns the number of missing values
# Check column names
colnames(data)
# Convert categorical variables to factors
data$Gender <- as.factor(data$Gender)
data$Hormonal.Changes <- as.factor(data$Hormonal.Changes)
data$Family.History <- as.factor(data$Family.History)
data$Race.Ethnicity <- as.factor(data$Race.Ethnicity)
data$Body.Weight <- as.factor(data$Body.Weight)
data$Calcium.Intake <- as.factor(data$Calcium.Intake)
data$Vitamin.D.Intake <- as.factor(data$Vitamin.D.Intake)
data$Physical.Activity <- as.factor(data$Physical.Activity)
data$Smoking <- as.factor(data$Smoking)
data$Alcohol.Consumption <- as.factor(data$Alcohol.Consumption)
data$Medications <- as.factor(data$Medications)
data$Prior.Fractures <- as.factor(data$Prior.Fractures)
# Convert the target variable (Osteoporosis) into a factor
# Replace numerical values (1, 0) with "Yes" and "No"
data$Osteoporosis <- ifelse(data$Osteoporosis == 1, "Yes", "No")
data$Osteoporosis <- as.factor(data$Osteoporosis)
# Split the dataset into training (70%) and validation (30%) sets
set.seed(123) # Set seed for reproducibility
trainIndex <- createDataPartition(data$Osteoporosis, p = 0.7, list = FALSE)
trainData <- data[trainIndex, ] # Training dataset
valData <- data[-trainIndex, ] # Validation dataset
# Check the distribution of the target variable in both datasets
table(trainData$Osteoporosis) # Count of each class in training data
table(valData$Osteoporosis) # Count of each class in validation data
# Ensure that the levels of the target variable are consistent across both datasets
levels(valData$Osteoporosis) <- levels(trainData$Osteoporosis)
# Train a logistic regression model on the training dataset
model <- train(Osteoporosis ~ ., data = trainData, method = "glm", family = "binomial")
# Summarize the trained model
summary(model)
# Predict probabilities and classes for the validation dataset
probs <- predict(model, newdata = valData, type = "prob") # Generates probabilities
predictions <- predict(model, newdata = valData) # Class predictions
# Evaluate the model's performance using a confusion matrix
cm <- confusionMatrix(predictions, valData$Osteoporosis)
print("Confusion Matrix:")
print(cm) # Outputs the confusion matrix
# Convert the confusion matrix into a data frame for visualization
cm_df <- as.data.frame(as.table(cm))
colnames(cm_df) <- c("Predicted", "Actual", "Freq")
# Plot the confusion matrix
ggplot(cm_df, aes(x = Actual, y = Predicted, fill = Freq)) +
geom_tile(color = "white") +
geom_text(aes(label = Freq), color = "black", size = 4) +
scale_fill_gradient(low = "lightblue", high = "blue") +
labs(title = "Confusion Matrix", x = "Actual Class", y = "Predicted Class") +
theme_minimal()
# Generate and plot the ROC curve
roc_curve <- roc(valData$Osteoporosis, probs$Yes) # Generate ROC curve data
ggplot(data.frame(x = roc_curve$specificities, y = roc_curve$sensitivities), aes(x, y)) +
geom_line(color = "blue") +
labs(title = "ROC Curve", x = "1 - Specificity", y = "Sensitivity") +
theme_minimal()
# Argumentation of the ROC curve results:
# - A perfect model would have the curve reaching the top left corner (0, 1).
# - The steeper the curve, the better the model distinguishes between classes.
# - AUC is a summary measure of the ROC curve:
#   - AUC = 0.5: Random guessing.
#   - AUC = 1: Perfect classification.
# Generate the Precision-Recall Curve
pr_curve <- pr.curve(scores.class0 = probs$Yes, weights.class0 = as.numeric(valData$Osteoporosis) - 1, curve = TRUE)
# Convert PR curve data to a data frame
df <- data.frame(
recall = pr_curve$curve[,1],
precision = pr_curve$curve[,2]
)
# Plot the Precision-Recall Curve
ggplot(df, aes(x = recall, y = precision)) +
geom_line(color = "red", size = 1) +
labs(title = "Precision-Recall Curve", x = "Recall", y = "Precision") +
theme_minimal()
# Feature Importance: Identify the most influential variables
# Train a Random Forest model to extract feature importance
modelRF <- randomForest(Osteoporosis ~ ., data = trainData)
# Extract and visualize feature importance
importance_values <- importance(modelRF)
importance_df <- data.frame(Feature = rownames(importance_values), Importance = importance_values[, 1])
# Plot feature importance using ggplot2
ggplot(importance_df, aes(x = reorder(Feature, Importance), y = Importance)) +
geom_bar(stat = "identity", fill = "steelblue") +
coord_flip() + # Flips the x and y axes for better readability
labs(title = "Feature Importance", x = "Feature", y = "Importance") +
theme_minimal()
# Save the trained models for future use
save(model, file = paste0(path, "logistic_model.RData"))
#Install required packages for data handling, model training, and evaluation
#install.packages("PRROC")
#install.packages("pROC")
#install.packages("smotefamily")
#install.packages("randomForest")
# Load the necessary libraries
library(caret)    # For model training and validation
library(dplyr)     # For data manipulation
library(smotefamily) # For oversampling techniques
library(ggplot2) # For data visualization
library(pROC)    # For ROC curve analysis
library(randomForest)# For Random Forest algorithm
library(PRROC)    # For Precision-Recall Curves
# Load the dataset ensuring that is saved at the specified path
data <- read.csv("osteoporosis.csv")
# Remove irrelevant columns that do not contribute to prediction such as "Id
data <- data %>% select(-Id, -Medical.Conditions)
# Preview the first few rows of the dataset
head(data)
# Check for missing values in the dataset
sum(is.na(data)) # Returns the number of missing values
# Check column names
colnames(data)
# Convert categorical variables to factors
data$Gender <- as.factor(data$Gender)
data$Hormonal.Changes <- as.factor(data$Hormonal.Changes)
data$Family.History <- as.factor(data$Family.History)
data$Race.Ethnicity <- as.factor(data$Race.Ethnicity)
data$Body.Weight <- as.factor(data$Body.Weight)
data$Calcium.Intake <- as.factor(data$Calcium.Intake)
data$Vitamin.D.Intake <- as.factor(data$Vitamin.D.Intake)
data$Physical.Activity <- as.factor(data$Physical.Activity)
data$Smoking <- as.factor(data$Smoking)
data$Alcohol.Consumption <- as.factor(data$Alcohol.Consumption)
data$Medications <- as.factor(data$Medications)
data$Prior.Fractures <- as.factor(data$Prior.Fractures)
# Convert the target variable (Osteoporosis) into a factor
# Replace numerical values (1, 0) with "Yes" and "No"
data$Osteoporosis <- ifelse(data$Osteoporosis == 1, "Yes", "No")
data$Osteoporosis <- as.factor(data$Osteoporosis)
# Split the dataset into training (70%) and validation (30%) sets
set.seed(123) # Set seed for reproducibility
trainIndex <- createDataPartition(data$Osteoporosis, p = 0.7, list = FALSE)
trainData <- data[trainIndex, ] # Training dataset
valData <- data[-trainIndex, ] # Validation dataset
# Check the distribution of the target variable in both datasets
table(trainData$Osteoporosis) # Count of each class in training data
table(valData$Osteoporosis) # Count of each class in validation data
# Ensure that the levels of the target variable are consistent across both datasets
levels(valData$Osteoporosis) <- levels(trainData$Osteoporosis)
# Train a logistic regression model on the training dataset
model <- train(Osteoporosis ~ ., data = trainData, method = "glm", family = "binomial")
# Summarize the trained model
summary(model)
# Predict probabilities and classes for the validation dataset
probs <- predict(model, newdata = valData, type = "prob") # Generates probabilities
predictions <- predict(model, newdata = valData) # Class predictions
# Evaluate the model's performance using a confusion matrix
cm <- confusionMatrix(predictions, valData$Osteoporosis)
print("Confusion Matrix:")
print(cm) # Outputs the confusion matrix
# Convert the confusion matrix into a data frame for visualization
cm_df <- as.data.frame(as.table(cm))
colnames(cm_df) <- c("Predicted", "Actual", "Freq")
# Plot the confusion matrix
ggplot(cm_df, aes(x = Actual, y = Predicted, fill = Freq)) +
geom_tile(color = "white") +
geom_text(aes(label = Freq), color = "black", size = 4) +
scale_fill_gradient(low = "lightblue", high = "blue") +
labs(title = "Confusion Matrix", x = "Actual Class", y = "Predicted Class") +
theme_minimal()
# Generate and plot the ROC curve
roc_curve <- roc(valData$Osteoporosis, probs$Yes) # Generate ROC curve data
ggplot(data.frame(x = roc_curve$specificities, y = roc_curve$sensitivities), aes(x, y)) +
geom_line(color = "blue") +
labs(title = "ROC Curve", x = "1 - Specificity", y = "Sensitivity") +
theme_minimal()
# Argumentation of the ROC curve results:
# - A perfect model would have the curve reaching the top left corner (0, 1).
# - The steeper the curve, the better the model distinguishes between classes.
# - AUC is a summary measure of the ROC curve:
#   - AUC = 0.5: Random guessing.
#   - AUC = 1: Perfect classification.
# Generate the Precision-Recall Curve
pr_curve <- pr.curve(scores.class0 = probs$Yes, weights.class0 = as.numeric(valData$Osteoporosis) - 1, curve = TRUE)
# Convert PR curve data to a data frame
df <- data.frame(
recall = pr_curve$curve[,1],
precision = pr_curve$curve[,2]
)
# Plot the Precision-Recall Curve
ggplot(df, aes(x = recall, y = precision)) +
geom_line(color = "red", size = 1) +
labs(title = "Precision-Recall Curve", x = "Recall", y = "Precision") +
theme_minimal()
# Feature Importance: Identify the most influential variables
# Train a Random Forest model to extract feature importance
modelRF <- randomForest(Osteoporosis ~ ., data = trainData)
# Extract and visualize feature importance
importance_values <- importance(modelRF)
importance_df <- data.frame(Feature = rownames(importance_values), Importance = importance_values[, 1])
# Plot feature importance using ggplot2
ggplot(importance_df, aes(x = reorder(Feature, Importance), y = Importance)) +
geom_bar(stat = "identity", fill = "steelblue") +
coord_flip() + # Flips the x and y axes for better readability
labs(title = "Feature Importance", x = "Feature", y = "Importance") +
theme_minimal()
# Save the trained models for future use
save(model, file = "logistic_model.RData")
save(modelRF, file = "random_forest_model.RData")
# List files in the directory to confirm saved models
list.files()
